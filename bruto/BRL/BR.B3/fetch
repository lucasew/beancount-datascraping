#!/usr/bin/env python3

from pathlib import Path
from dataclasses import dataclass
from enum import Enum
from datetime import date, datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Optional
import re
import json
import logging
from urllib.request import urlopen, Request
import functools
from csv import DictWriter

logging.basicConfig()
logger = logging.getLogger(__name__)

CURRENT_DIR=Path(__file__).parent
TODAY = datetime.today().date()

USER_AGENT = "Mozilla/5.0 (X11; Linux x86_64; rv:137.0) Gecko/20100101 Firefox/137.0"

@functools.cache
def fetch_body(url: str, fetch_try=0):
    """
    Downloads raw payload from an endpoint and caches it via functools.
    Utilizes a hardcoded browser User-Agent to mitigate 403 blocks from CDNs.
    """
    logger.debug(f"fetch: {url}")
    res = urlopen(Request(url, headers={
      "User-Agent": USER_AGENT,
    }))
    return res.read()

def fetch_json(url: str):
    """
    Loads cached bytes returned by fetch_body and parses them into a Python dict.
    """
    return json.loads(fetch_body(url))

fetchers = {}
def fetcher(fn):
    fetchers[fn.__name__] = fn
    return fn

@dataclass
class AssetType(Enum):
    """
    Distinguishes the major classes of B3 assets. Used as the underlying file name
    and heavily influences how scrapers construct their target URLs.
    """
    ACAO = "acoes"
    FII = "fii"
    ETF = "etfs"

    def __hash__(self):
        return self.value.__hash__()

@dataclass
class Entry():
    """
    Encapsulates a parsed daily price metric. Serves as the canonical data
    structure emitted by any downstream scraping task.
    """
    day: date
    price: float

    @property
    def columns(self):
        return ['day', 'price']

    @property
    def line(self):
        return {"day": str(self.day), "price": self.price}

@dataclass
class Job():
    """
    Holds execution context for a single scrape. Defines what ticker to hit,
    what asset type it is, and via which scraping logic. Exposes the proper
    CSV persistence path based on this tuple of properties.
    """
    ticker: str
    type: AssetType
    source: str

    @property
    def output_file(self):
        ret = CURRENT_DIR / self.source / (self.ticker + ".csv")
        ret.parent.mkdir(parents=True, exist_ok=True)
        return ret

@dataclass
class Result():
    """
    The wrapper yielded by `handle_job` after execution attempts finish.
    A `None` entry indicates scraping fell through or an exception occurred.
    """
    entry: Optional[Entry]
    job: Job


@fetcher
def none(job: Job):
    return Result(job=job, entry=None)

@fetcher
def statusinvest(job: Job):
    """
    Scrapes StatusInvest's HTML page for the given ticker. Resolves the correct
    sub-path ('acoes', 'fundos-imobiliarios', etc.) based on the Job's AssetType.
    Uses regex to extract the "Valor atual" field since they don't expose a clean API.
    Transforms comma-separated prices to float-compatible dots.
    """
    kind = {
        AssetType.ACAO: 'acoes',
        AssetType.FII: 'fundos-imobiliarios',
        AssetType.ETF: 'etfs'
    }[job.type]
    url = f"https://statusinvest.com.br/{kind}/{job.ticker}"
    content = fetch_body(url).decode('utf-8')
    regex = r"\<div title=\"Valor atual do ativo\"\>\s*\<h3 class=\"title m-0\">Valor atual\</h3\>\s*\<span class=\"icon\"\>(R\$)\</span\>\s*\<strong class=\"value\"\>([0-9]*,[0-9]{2})\</strong\>\s*\</div\>"
    for item in re.finditer(regex, content, re.MULTILINE):
        value = item.group(2).replace('.', '').replace(',', '.')
        return Result(job=job, entry=Entry(day=TODAY, price=value))
    return Result(job=job, entry=None)

def handle_job(job: Job):
    """
    Dispatches the job to its configured extractor function. Acts as a safety net
    inside the ThreadPoolExecutor, suppressing individual job crashes so the
    rest of the pool can continue unabated.
    """
    try:
        return fetchers[job.source](job)
    except Exception as e:
        logger.error(e)
        return Result(job=job, entry=None)

def get_jobs():
    """
    Bootstraps the processing queue by iterating over `.txt` manifest files.
    Each file indicates an asset category (e.g. `acoes.txt`).
    Pairs every valid ticker with every available fetching source.
    """
    for item in CURRENT_DIR.glob('*.txt'):
        asset_type = AssetType(item.stem)
        for ticker in item.open('r'):
            ticker = ticker.strip()
            if ticker == '':
                continue
            for f in fetchers.keys():
                yield Job(ticker=ticker, type=asset_type, source=f)

with ThreadPoolExecutor(max_workers=8) as tp:
    for result in tp.map(handle_job, get_jobs()):
        if result.entry is None:
            continue
        output_file: Path = result.job.output_file
        if not output_file.exists():
            with output_file.open('w') as f:
                w = DictWriter(f, fieldnames=result.entry.columns)
                w.writeheader()
        with output_file.open('a') as f:
            w = DictWriter(f, fieldnames=result.entry.columns)
            w.writerow(result.entry.line)
        print(result)

