#!/usr/bin/env python3

from pathlib import Path
from dataclasses import dataclass
from enum import Enum
from datetime import date, datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Optional
import re
import json
import logging
from urllib.request import urlopen, Request
import functools
from csv import DictWriter

logging.basicConfig()
logger = logging.getLogger(__name__)

CURRENT_DIR=Path(__file__).parent
TODAY = datetime.today().date()

USER_AGENT = "Mozilla/5.0 (X11; Linux x86_64; rv:137.0) Gecko/20100101 Firefox/137.0"

@functools.cache
def fetch_body(url: str, fetch_try=0):
    """
    Fetches raw body content from a URL, spoofing a standard browser user agent
    to avoid basic bot protections. Caches results in memory to prevent duplicate
    network requests for the same URL during a single script execution.
    """
    logger.debug(f"fetch: {url}")
    res = urlopen(Request(url, headers={
      "User-Agent": USER_AGENT,
    }))
    return res.read()

def fetch_json(url: str):
    """
    Wraps fetch_body to directly decode the fetched payload as JSON.
    Benefits from the underlying memory cache in fetch_body.
    """
    return json.loads(fetch_body(url))

fetchers = {}
def fetcher(fn):
    fetchers[fn.__name__] = fn
    return fn

@dataclass
class AssetType(Enum):
    """
    Defines the supported financial asset categories. Used to differentiate
    asset lists (e.g., from 'etf.txt') and dictate fetching logic if needed.
    """
    ETF = "etf"

    def __hash__(self):
        return self.value.__hash__()

@dataclass
class Entry():
    """
    Represents a single scraped data point for a ticker. Designed
    to map easily to a CSV row format representing time series data.
    """
    day: date
    price: float

    @property
    def columns(self):
        return ['day', 'price']

    @property
    def line(self):
        return {"day": str(self.day), "price": self.price}

@dataclass
class Job():
    """
    Encapsulates the context needed to process an individual asset,
    including the ticker symbol, its asset type, and the target fetching source.
    Generates the correct output file path dynamically based on these parameters.
    """
    ticker: str
    type: AssetType
    source: str

    @property
    def output_file(self):
        ret = CURRENT_DIR / self.source / (self.ticker + ".csv")
        ret.parent.mkdir(parents=True, exist_ok=True)
        return ret

@dataclass
class Result():
    """
    Binds the outcome of a scraping job to its original request.
    If `entry` is None, the job failed or yielded no data.
    """
    entry: Optional[Entry]
    job: Job


@fetcher
def none(job: Job):
    return Result(job=job, entry=None)

@fetcher
def etfdotcom(job: Job):
    """
    Data extractor targeting ETF.com. Calls their internal v2 delayed quotes API.
    Assumes the API returns a list and parses out the 'Last' traded price.
    """
    url = f"https://api-prod.etf.com//v2/quotes/delayedquotes?tickers={job.ticker}"
    content = fetch_json(url)
    for item in content:
        return Result(job=job, entry=Entry(TODAY, item['Last']))
    return Result(job=job, entry=None)


def handle_job(job: Job):
    """
    Worker function executed inside the thread pool. Looks up the registered
    fetcher function by name and invokes it. Catches all exceptions to prevent
    a single failed scrape from crashing the entire batch process.
    """
    try:
        return fetchers[job.source](job)
    except Exception as e:
        logger.error(e)
        return Result(job=job, entry=None)

def get_jobs():
    """
    Generator that orchestrates job creation by scanning directory conventions.
    Reads flat .txt files in the current dir (e.g. 'etf.txt'). The file stem
    dictates the asset type. Emits a scrape job for every combination of
    discovered ticker and registered fetcher.
    """
    for item in CURRENT_DIR.glob('*.txt'):
        asset_type = AssetType(item.stem)
        for ticker in item.open('r'):
            ticker = ticker.strip()
            if ticker == '':
                continue
            for f in fetchers.keys():
                yield Job(ticker=ticker, type=asset_type, source=f)

with ThreadPoolExecutor(max_workers=8) as tp:
    for result in tp.map(handle_job, get_jobs()):
        if result.entry is None:
            continue
        output_file: Path = result.job.output_file
        if not output_file.exists():
            with output_file.open('w') as f:
                w = DictWriter(f, fieldnames=result.entry.columns)
                w.writeheader()
        with output_file.open('a') as f:
            w = DictWriter(f, fieldnames=result.entry.columns)
            w.writerow(result.entry.line)
        print(result)

